#####################################################################################
The binarized outcome of the input layer:  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
         1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
         1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
         1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0')
#####################################################################################
first layer output tensor([[  2.,  34., -22.,  16., -16.,   2., -16.,   6.,  32., -24.,  -4., -24.,
           8.,  -4.,   2., -10.,  -4.,   4.,   6.,  -4., -32., -16.,   4., -16.,
         -26.,   2., -12., -22.,   4., -20.,  26.,   0., -14.,  12.,  22.,  30.,
         -34., -12., -14.,   2., -22.,  -8.,  -2.,   4., -22.,  24.,  10.,  38.,
          16.,   6.,   2.,  14.,   0., -10.,  -2.,  18.,   2.,   0.,  30.,  -8.,
           4., -12.,  26.,   4.]], device='cuda:0', grad_fn=<AddBackward0>)
BatchNorm1d running mean: tensor([-13.9704,  16.9761, -15.6330,   4.8947, -19.1074,  12.1394,   0.1556,
          4.2703,  23.9676,   1.3335,   8.1710, -15.3167,  -8.3775, -12.8122,
         -0.6914, -10.1087, -12.0643, -18.6084,  34.1970,  -6.5134,   5.9569,
          7.4978,  -1.6856, -19.7400,  -9.1361,  -0.9957,  -6.3896,   5.2126,
          5.3433,  -3.0539,  -3.9027,   4.7467,   3.7783,  10.3042,   3.7267,
         11.3387,  -9.2913, -11.2780,  12.9040,   0.0617,  -0.5948,  -8.9296,
          8.1448,   1.5899,  -7.2445,   5.5884,  19.8732,   9.1746,   8.4129,
          1.4789,   9.8116,  -3.3499, -10.6523,   1.6005,   7.6332,   0.3051,
          5.7470,   9.8094,  10.5161,  -2.9694,  -8.9005, -16.5370,   3.6797,
        -16.6013], device='cuda:0')
after the first activate function: tensor([[ 7.9113e-01,  1.0000e+00, -3.6022e-01,  1.0000e+00,  1.4030e-01,
         -9.0243e-01, -1.0000e+00,  1.3390e-01,  4.6555e-01, -1.0000e+00,
         -1.0000e+00, -3.7825e-01,  1.0000e+00,  8.1771e-01,  1.6815e-03,
          6.5712e-03,  7.8961e-01,  8.4529e-03, -7.4289e-03,  1.7841e-01,
         -1.0000e+00, -8.1503e-01,  4.3909e-01,  2.3883e-01, -1.0000e+00,
          1.4785e-01, -3.3468e-01, -1.0000e+00, -8.0455e-02, -1.0000e+00,
          1.0000e+00, -4.4273e-01, -1.0000e+00,  1.7102e-01,  1.0000e+00,
          1.0000e+00, -1.0000e+00, -2.9112e-04, -1.0000e+00,  1.2079e-01,
         -1.0000e+00,  1.0320e-01, -5.6210e-01,  2.5290e-02, -8.8311e-01,
          1.0000e+00, -4.7257e-01,  1.0000e+00,  5.1533e-01,  3.4965e-01,
         -6.1984e-01,  1.0000e+00,  2.3121e-03, -1.0000e+00, -5.4977e-01,
          1.0000e+00, -3.3387e-01, -7.6044e-01,  1.0000e+00, -3.5901e-01,
          7.1756e-01,  1.9112e-01,  5.3766e-01,  8.0351e-01]], device='cuda:0',
       grad_fn=<HardtanhBackward0>)
#####################################################################################
The binarized outcome of this layer:  tensor([[ 1.,  1., -1.,  1.,  1., -1., -1.,  1.,  1., -1., -1., -1.,  1.,  1.,
          1.,  1.,  1.,  1., -1.,  1., -1., -1.,  1.,  1., -1.,  1., -1., -1.,
         -1., -1.,  1., -1., -1.,  1.,  1.,  1., -1., -1., -1.,  1., -1.,  1.,
         -1.,  1., -1.,  1., -1.,  1.,  1.,  1., -1.,  1.,  1., -1., -1.,  1.,
         -1., -1.,  1., -1.,  1.,  1.,  1.,  1.]], device='cuda:0',
       grad_fn=<Binarize2Backward>)
#####################################################################################
second layer output tensor([[-16.,   2., -12., -26.,  -6.,   6.,   2., -20.,  10.,   6.,  14.,   4.,
         -22., -16., -10., -14., -10.,   8., -20.,  16.,   4., -16.,  12., -10.,
         -10.,  20.,   6.,  20.,  -4., -10.,  26.,  12.]], device='cuda:0',
       grad_fn=<AddBackward0>)
BatchNorm1d running mean: tensor([-0.6925, -0.8401, -1.4193, -1.4612,  0.1325,  0.4269,  0.3677, -0.4077,
         0.7301,  0.6798,  0.8632,  0.5477, -1.9522, -0.6920, -1.2653, -0.9486,
        -1.0647,  0.3745, -1.6884,  1.4703,  0.5251,  0.1731,  0.4473, -0.2385,
        -0.2538,  0.8427,  0.6120,  1.0866, -1.2465, -0.3488,  0.2732, -0.3749],
       device='cuda:0')
after the second activate function: tensor([[-1.0000,  0.2239, -1.0000, -0.0108, -0.5209,  0.5378,  0.2172, -1.0000,
          0.8846,  0.2139,  0.5950,  0.2241, -1.0000, -1.0000, -1.0000, -1.0000,
         -0.7776,  0.6663, -1.0000,  1.0000,  0.2544, -1.0000,  1.0000,  0.0012,
         -0.0195,  1.0000,  0.5161,  1.0000, -0.1698, -0.5985,  1.0000,  1.0000]],
       device='cuda:0', grad_fn=<HardtanhBackward0>)
#####################################################################################
The binarized outcome of this layer:  tensor([[-1.,  1., -1., -1., -1.,  1.,  1., -1.,  1.,  1.,  1.,  1., -1., -1.,
         -1., -1., -1.,  1., -1.,  1.,  1., -1.,  1.,  1., -1.,  1.,  1.,  1.,
         -1., -1.,  1.,  1.]], device='cuda:0', grad_fn=<Binarize2Backward>)
#####################################################################################
third layer output tensor([[  6., -12.,   0.,   4.,  -8.,   8.,   0.,   0.,  -4.,  -2.,   8.,  12.,
           0.,   4.,   4.,  -8.,  14.,  -6.,  16.,  -4.,  12.,  12.,  10.,   4.,
           4.,   2.,  -4.,   0.,   8., -12.,  -2.,   6.]], device='cuda:0',
       grad_fn=<AddBackward0>)
BatchNorm1d running mean: tensor([-0.1421, -0.4044,  0.0382,  0.4523, -0.8855,  0.0148,  0.3443, -0.1808,
         0.1275, -0.1793,  0.0287,  0.3350,  0.1180,  0.0182, -0.2080, -0.1569,
         0.4483, -0.2264,  0.2487,  0.1147,  0.6496, -0.1060, -0.0182,  0.0729,
         0.1367,  0.2977, -0.3941,  0.2871,  1.1665, -0.3890,  0.0599,  0.5606],
       device='cuda:0')
after the third activate function: tensor([[ 0.6389, -0.9059, -0.0069,  0.3622, -1.0000,  0.8886, -0.0397,  0.0321,
         -0.0102, -0.0680,  1.0000,  1.0000, -0.0190,  0.6435,  0.5712, -1.0000,
          1.0000, -0.0129,  1.0000, -0.5256,  1.0000,  1.0000,  1.0000,  0.7877,
          0.3391,  0.1936, -0.7040, -0.0363,  1.0000, -1.0000, -0.2035,  0.5169]],
       device='cuda:0', grad_fn=<HardtanhBackward0>)
#####################################################################################
The binarized outcome of this layer:  tensor([[ 1., -1., -1.,  1., -1.,  1., -1.,  1., -1., -1.,  1.,  1., -1.,  1.,
          1., -1.,  1., -1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1., -1., -1.,
          1., -1., -1.,  1.]], device='cuda:0', grad_fn=<Binarize2Backward>)
#####################################################################################
the fourth layer output: tensor([[-10.,  20.,   0.,  -8.,  -6.,  -8.,   4.,   6.,  -8.,   0.]],
       device='cuda:0', grad_fn=<AddBackward0>)
torch.Size([1, 28, 28])
prediction: 1
target: 1
##################################################################################
target:  tensor([1, 3, 4,  ..., 2, 4, 1])
##################################################################################
output: tensor([[9.3576e-14, 1.0000e+00, 2.0612e-09, 6.9144e-13, 5.1091e-12, 6.9144e-13,
         1.1254e-07, 8.3153e-07, 6.9144e-13, 2.0612e-09]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
##################################################################################
bf_softmax_output: tensor([[-10.,  20.,   0.,  -8.,  -6.,  -8.,   4.,   6.,  -8.,   0.]],
       grad_fn=<ToCopyBackward0>)
##################################################################################
